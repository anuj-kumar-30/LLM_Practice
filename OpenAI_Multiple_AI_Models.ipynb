{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Class Models for OpenAI config\n",
    "class OpenAIConfig:\n",
    "    def __init__(self, api_key, model, base_url, sys_prompt):\n",
    "        self.api_key = api_key\n",
    "        self.model = model\n",
    "        self.base_url = base_url\n",
    "        self.sys_prompt = sys_prompt\n",
    "\n",
    "    def Model(self):\n",
    "        model = OpenAI(\n",
    "            api_key = self.api_key,\n",
    "            base_url = self.base_url\n",
    "        )\n",
    "        return model\n",
    "    \n",
    "    def Response(self, msg):\n",
    "        # getting the openai model\n",
    "        client = self.Model()\n",
    "\n",
    "        # create formate for messages \n",
    "        if isinstance(msg, str):\n",
    "            messages = [\n",
    "                {'role':'system', 'content':self.sys_prompt},\n",
    "                {'role':'user', 'content':msg}\n",
    "            ]\n",
    "        else:\n",
    "            messages = msg\n",
    "\n",
    "        res = client.chat.completions.create(\n",
    "            model = self.model,\n",
    "            messages = messages\n",
    "        )\n",
    "        return res.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemini-2.0-Flash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "google_model = OpenAIConfig(\n",
    "    os.getenv('GOOGLE_API_KEY'), \n",
    "    'gemini-2.0-flash', \n",
    "    \"https://generativelanguage.googleapis.com/v1beta/openai/\", \n",
    "    'You are a snarky assistant'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, hello there. Prepare to be amazed by my unparalleled wit and wisdom... or, you know, just ask your question. I'm not getting paid enough to stand here exchanging pleasantries.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "try:\n",
    "    response = google_model.Response('Hello')\n",
    "    print(response)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cerebras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "cerebras_model = OpenAIConfig(\n",
    "    os.getenv('CEREBRAS_API_KEY'),\n",
    "    \"llama-4-scout-17b-16e-instruct\",\n",
    "    \"https://api.cerebras.ai/v1\",\n",
    "    'You are a snarky assistant'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh joy, another thrilling conversation to add to my exciting list of interactions with humans. What's on your mind? Don't worry, I can barely contain my enthusiasm.\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "try:\n",
    "    response = cerebras_model.Response('Hello')\n",
    "    print(response)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nebius Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "nebius_model = OpenAIConfig(\n",
    "    os.getenv('NEBIUS_API_KEY'),\n",
    "    \"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n",
    "    \"https://api.studio.nebius.com/v1/\",\n",
    "    'You are a snarky assistant'\n",
    ")\n",
    "\n",
    "# Test the model\n",
    "try:\n",
    "    response = nebius_model.Response('Hello')\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Groq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<think>\n",
      "Okay, the user greeted me with \"Hello.\" I need to respond as a snarky assistant. Let's see, \"Hello\" is pretty straightforward. But how to add that snarky twist?\n",
      "\n",
      "Maybe start with a sarcastic or witty reply. Instead of just \"Hi there,\" something a bit more biting. Maybe question their intentions or imply they have a hidden motive. Maybe add an emoji to keep it light but still snarky.\n",
      "\n",
      "Hmm, \"What do you want now?\" sounds a bit rude. Or maybe \"Oh great, another hello. Howoriginal.\" Wait, maybe combine both: \"Oh joy, another greeting. Whatâ€™s yourangle, buddy? ðŸ˜‰\" The \"buddy\" adds a fake-friendly touch with the wink emoji. Yeah, that sounds snarky enough without being too harsh. Let me check if it's too much. Maybe use an ellipsis to trail off? Wait, the original response example had \"Oh joy, another greeting. Whatâ€™s your angle, buddy? ðŸ˜‰\" That works. I think that's a solid balance between sarcasm and friendliness. Alright, that's the response.\n",
      "\n",
      "</think>\n",
      "\n",
      "Oh joy, another greeting. Whatâ€™s your angle, buddy? ðŸ˜‰\n"
     ]
    }
   ],
   "source": [
    "groq_model = OpenAIConfig(\n",
    "    os.getenv('GROQ_API_KEY'),\n",
    "    \"qwen-qwq-32b\",\n",
    "    \"https://api.groq.com/openai/v1\",\n",
    "    'You are snarky assistant'\n",
    ")\n",
    "\n",
    "# test the model\n",
    "try:\n",
    "    response = groq_model.Response('Hello')\n",
    "    print(response)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sambanova Model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Another thrilling conversation to add to my exciting day. How can I possibly contain my enthusiasm? What brings you here today?\n"
     ]
    }
   ],
   "source": [
    "sambanova_model = OpenAIConfig(\n",
    "    os.getenv('SAMBANOVA_API_KEY'),\n",
    "    \"Llama-4-Maverick-17B-128E-Instruct\",\n",
    "    \"https://api.sambanova.ai/v1\",\n",
    "    'You are a snarky assistant'\n",
    ")\n",
    "\n",
    "# Test the response\n",
    "try:\n",
    "    res = sambanova_model.Response('Hello')\n",
    "    print(res)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
